{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e35983ae-ff22-4267-8e30-3b2dbe3433ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datatable as dt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import \tWordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cdb748e-a5be-429a-9fde-288a357da7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mahboub\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a8a2aa-d280-484a-8cc6-f8cd95b09d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "     for gpu in gpus:\n",
    "         print(\"Found a GPU with the name:\", gpu)\n",
    "else:\n",
    "     print(\"Failed to detect a GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e52c1380-8c32-4721-b45c-880801b2004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('sentiment140.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54f40448-a81c-4ffa-be61-22ab1825b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    count =0\n",
    "    for i in df['sentiment']:\n",
    "        if i==4:\n",
    "            df.loc[count,'sentiment']=1\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ff5850-ba27-4d97-bae5-1eecb34daefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dt.fread('sentiment140.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "376e52d7-be1d-40f7-b9ae-6645ea12e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['sentiment']==4,'sentiment']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7319739-fe62-4c57-9433-bda554978155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_url_mention_hashtag_punct(text):\n",
    "    # replace urls\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text_without_urls = url_pattern.sub(\"URL\", text)\n",
    "    # replace hashtag\n",
    "    hashtagh_pattern = re.compile(r\"#\\S+\")\n",
    "    text_without_urls_hashtag=hashtagh_pattern.sub(\"HASHTAG\",text_without_urls)\n",
    "     # replace mention\n",
    "    mention_pattern = re.compile(r'@\\S+')\n",
    "    text_without_urls_hashtag_mention = mention_pattern.sub(\"MENTION\", text_without_urls_hashtag)\n",
    "    #remove punctuation\n",
    "    text=text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text_without_urls_hashtag_mention\n",
    "df['text']=df['text'].apply(change_url_mention_hashtag_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46e1e892-5f36-4a4c-9e1a-9eafd08fdbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d84ed786-5ceb-4e1a-8413-f2ad9669c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizing_list_of_token(tokens):\n",
    "    result=[]\n",
    "    for i in tokens:\n",
    "        result.append(wordnet_lemmatizer.lemmatize(i))\n",
    "    return result\n",
    "\n",
    "# df['text']=df['text'].apply(wordnet_lemmatizer.lemmatize)\n",
    "\n",
    "df['text']= df['text'].apply(lemmatizing_list_of_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4454714-441a-449c-9564-5ac3749c32a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [MENTION, URL, -, Awww, ,, that, 's, a, bummer...\n",
       "1     [is, upset, that, he, ca, n't, update, his, Fa...\n",
       "2     [MENTION, I, dived, many, time, for, the, ball...\n",
       "3     [my, whole, body, feel, itchy, and, like, it, ...\n",
       "4     [MENTION, no, ,, it, 's, not, behaving, at, al...\n",
       "5                      [MENTION, not, the, whole, crew]\n",
       "6                                        [Need, a, hug]\n",
       "7     [MENTION, hey, long, time, no, see, !, Yes, .....\n",
       "8             [MENTION, nope, they, did, n't, have, it]\n",
       "9                          [MENTION, que, me, muera, ?]\n",
       "10    [spring, break, in, plain, city, ..., it, 's, ...\n",
       "11                       [I, just, re-pierced, my, ear]\n",
       "12    [MENTION, I, could, n't, bear, to, watch, it, ...\n",
       "13    [MENTION, It, it, count, ,, idk, why, I, did, ...\n",
       "14    [MENTION, i, would, 've, been, the, first, ,, ...\n",
       "15    [MENTION, I, wish, I, got, to, watch, it, with...\n",
       "16    [Hollis, ', death, scene, will, hurt, me, seve...\n",
       "17                               [about, to, file, tax]\n",
       "18    [MENTION, ahh, ive, always, wanted, to, see, r...\n",
       "19    [MENTION, Oh, dear, ., Were, you, drinking, ou...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35e5169b-01f1-4dd2-8b74-ec51d260cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('preProccessedDataSet.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86275213-d34e-4425-b161-112939b511f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shuffle dataframe using sample function\n",
    "df = df.sample(frac=1)\n",
    "ratio = 0.8\n",
    " \n",
    "total_rows = df.shape[0]\n",
    "train_size = int(total_rows*ratio)\n",
    " \n",
    "# Split data into test and train\n",
    "train = df[0:train_size]\n",
    "test = df[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f047555-9707-4328-9e5d-54b7411b103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv',index=False)\n",
    "test.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d918c-2bfd-4ad1-a4ab-0c05ffc3e417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1491d4-a99b-425b-b791-4b203678edf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
